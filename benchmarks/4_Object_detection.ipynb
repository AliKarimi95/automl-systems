{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import requirements"
      ],
      "metadata": {
        "id": "aYOcY9wcfhHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APp6N8zz7zV4"
      },
      "outputs": [],
      "source": [
        "# !pip3 install -U pip\n",
        "# !pip3 install -U setuptools wheel\n",
        "\n",
        "# Install the proper version of PyTorch following https://pytorch.org/get-started/locally/\n",
        "# !pip3 install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchtext==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "!pip3 install autogluon\n",
        "\n",
        "# For GPU users, CUDA 101\n",
        "!pip3 install \"mxnet_cu112<2.0.0, >=1.7.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ObjectDetector\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "1F9k565q76w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mOPvDDXNVeTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "ncdLA3cLfnsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "SxnO5-_a1UWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your kaggle API token into files panel (cwd), then run the cell\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "DfcA7A6Cm-_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r datasets\n",
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "4N4yi5JznYpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection?select=train_zip\n",
        "\n",
        "!kaggle datasets download -d mbkinaci/fruit-images-for-object-detection -p datasets --unzip"
      ],
      "metadata": {
        "id": "k-GfTSErnajW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove corrupted files\n",
        "\n",
        "!rm datasets/train_zip/train/apple_17.*\n",
        "!rm datasets/train_zip/train/apple_1.*\n",
        "!rm datasets/train_zip/train/apple_20.*\n",
        "!rm datasets/train_zip/train/apple_28.*\n",
        "!rm datasets/train_zip/train/apple_37.*\n",
        "!rm datasets/train_zip/train/apple_42.*\n",
        "!rm datasets/train_zip/train/apple_43.*\n",
        "!rm datasets/train_zip/train/apple_44.*\n",
        "!rm datasets/train_zip/train/apple_8.*\n",
        "!rm datasets/train_zip/train/banana_25.*\n",
        "!rm datasets/train_zip/train/banana_35.*\n",
        "!rm datasets/train_zip/train/banana_53.*\n",
        "!rm datasets/train_zip/train/banana_57.*\n",
        "!rm datasets/train_zip/train/banana_60.*\n",
        "!rm datasets/train_zip/train/banana_71.*\n",
        "!rm datasets/train_zip/train/banana_72.*\n",
        "!rm datasets/train_zip/train/banana_75.*\n",
        "!rm datasets/train_zip/train/mixed_16.*\n",
        "!rm datasets/train_zip/train/orange_11.*\n",
        "!rm datasets/train_zip/train/orange_13.*\n",
        "!rm datasets/train_zip/train/orange_18.*\n",
        "!rm datasets/train_zip/train/orange_1.*\n",
        "!rm datasets/train_zip/train/orange_22.*\n",
        "!rm datasets/train_zip/train/orange_30.*\n",
        "!rm datasets/train_zip/train/orange_35.*\n",
        "!rm datasets/train_zip/train/orange_42.*\n",
        "!rm datasets/train_zip/train/orange_4.*\n",
        "!rm datasets/train_zip/train/orange_50.*\n",
        "!rm datasets/train_zip/train/orange_51.*\n",
        "!rm datasets/train_zip/train/orange_59.*\n",
        "!rm datasets/train_zip/train/orange_64.*\n",
        "!rm datasets/train_zip/train/orange_68.*\n",
        "!rm datasets/train_zip/train/orange_70.*\n",
        "\n",
        "!rm datasets/test_zip/test/apple_79.*\n",
        "!rm datasets/test_zip/test/apple_92.*\n",
        "!rm datasets/test_zip/test/banana_87.*\n",
        "!rm datasets/test_zip/test/orange_94.*"
      ],
      "metadata": {
        "id": "dYvAZb_e3u1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(ObjectDetector.Dataset.from_voc)"
      ],
      "metadata": {
        "id": "m4kXRwZBoAXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r datasets/fruits\n",
        "!mkdir --parents datasets/fruits/Annotations\n",
        "!mkdir --parents datasets/fruits/ImageSets/Main\n",
        "!mkdir --parents datasets/fruits/JPEGImages"
      ],
      "metadata": {
        "id": "325z1rBJvPVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls datasets/test_zip/test/*.jpg  | xargs -n 1 basename | while read f; do echo ${f%.jpg}; done > datasets/fruits/ImageSets/Main/test.txt\n",
        "!ls datasets/train_zip/train/*.jpg  | xargs -n 1 basename | while read f; do echo ${f%.jpg}; done > datasets/fruits/ImageSets/Main/train.txt"
      ],
      "metadata": {
        "id": "nA1p49hz0HGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp datasets/{test_zip/test,train_zip/train}/*.jpg datasets/fruits/JPEGImages\n",
        "!cp datasets/{test_zip/test,train_zip/train}/*.xml datasets/fruits/Annotations"
      ],
      "metadata": {
        "id": "Oyw1Nnk3yv6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset objects"
      ],
      "metadata": {
        "id": "xmcsajk-og6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_bounding_boxes(image_path, rois):\n",
        "    default_color = 'white'\n",
        "    colors_map = {\n",
        "        'motorbike': 'green',\n",
        "        'person': 'red',\n",
        "        'car': 'blue',\n",
        "        'apple': 'red',\n",
        "        'orange': 'orange',\n",
        "        'banana': 'yellow',\n",
        "    }\n",
        "    get_color = lambda cls: colors_map.get(cls, default_color)\n",
        "\n",
        "    img = mpimg.imread(image_path)\n",
        "    h, w, c = img.shape\n",
        "    abs_roi = [[\n",
        "        int(roi['xmin'] * w),\n",
        "        int(roi['ymin'] * h),\n",
        "        int(roi['xmax'] * w),\n",
        "        int(roi['ymax'] * h),\n",
        "    ] for roi in rois]\n",
        "    colors = [get_color(roi['class']) for roi in rois]\n",
        "\n",
        "    # read input image from your computer\n",
        "    annot_img = read_image(image_path)\n",
        "    \n",
        "    # bounding box are xmin, ymin, xmax, ymax\n",
        "    box = torch.tensor(abs_roi, dtype=torch.int)\n",
        "    \n",
        "    # draw bounding box and fill color\n",
        "    annot_img = draw_bounding_boxes(\n",
        "        annot_img, box, width=3,\n",
        "        colors=colors, fill=False\n",
        "    )\n",
        "    \n",
        "    # transform this image to PIL image\n",
        "    annot_img = torchvision.transforms.ToPILImage()(annot_img)\n",
        "    \n",
        "    # display output\n",
        "    return annot_img"
      ],
      "metadata": {
        "id": "ppZQKAmwgBti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, name, path, train_split='train', test_split='test'):\n",
        "        self.name = name\n",
        "\n",
        "        self.train_data = ObjectDetector.Dataset.from_voc(path, splits=train_split)\n",
        "        print(f'[{name}] Train data size: {len(self.train_data)}')\n",
        "        self.test_data = ObjectDetector.Dataset.from_voc(path, splits=test_split)\n",
        "        print(f'[{name}] Test data size: {len(self.test_data)}')\n",
        "\n",
        "    def show_random_sample(self, split='train'):\n",
        "        data = self.train_data if split == 'train' else self.test_data\n",
        "        row = data.iloc[random.randint(0, len(data))]\n",
        "        return show_bounding_boxes(row['image'], row['rois'])"
      ],
      "metadata": {
        "id": "K6rGlzy2ogkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = []\n",
        "\n",
        "fruits_dataset_path = os.path.join('datasets', 'fruits')\n",
        "datasets.append(Dataset('fruits', fruits_dataset_path))\n",
        "\n",
        "tiny_morotbike_url = 'https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip'\n",
        "datasets.append(Dataset('motorbike', tiny_morotbike_url, train_split='trainval'))"
      ],
      "metadata": {
        "id": "k2bLeJ5_pgiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[0].show_random_sample('train')"
      ],
      "metadata": {
        "id": "_5eyTYrJp8BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[1].show_random_sample('train')"
      ],
      "metadata": {
        "id": "UEyOaaJWqbQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking"
      ],
      "metadata": {
        "id": "M5bbHUQKq0fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoGluonObjectDetectionAML:\n",
        "    def set_dataset(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        \n",
        "    def fit(self, \n",
        "            time_budget=60*15,    # at most 0.5 hour\n",
        "        ):\n",
        "        print(\n",
        "            f\"\"\"----------------------------\n",
        "            start fitting on {self.dataset.name}\n",
        "            ----------------------------\"\"\")\n",
        "        self.aml = ObjectDetector(\n",
        "            path=os.path.join('outputs', self.dataset.name)\n",
        "        )\n",
        "\n",
        "        # for demo \n",
        "        hyperparameters = {'epochs': 10, 'batch_size': 8}\n",
        "        hyperparameter_tune_kwargs={'num_trials': 5}\n",
        "        ##########\n",
        "\n",
        "        self.aml.fit(\n",
        "            self.dataset.train_data, \n",
        "            time_limit=time_budget, \n",
        "            hyperparameters=hyperparameters, \n",
        "            hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
        "            # presets='medium_quality_faster_train',\n",
        "        )\n",
        "\n",
        "        print('\\nTrain summary:\\n----------------------')\n",
        "        print(self.aml.fit_summary())\n",
        "    \n",
        "    def evaluate_test(self):\n",
        "        test_map = self.aml.evaluate(self.dataset.test_data)\n",
        "        print(\"mAP on `{}` test dataset: {}\".format(self.dataset.name, test_map[1][-1]))\n",
        "        return test_map\n",
        "\n",
        "    def predict_on_random_sample(self, confidence_threshold=0.7):\n",
        "        data = self.dataset.test_data\n",
        "        image_path = data.iloc[random.randint(0, len(data))]['image']\n",
        "        result = self.aml.predict(image_path)\n",
        "        result = result[result['predict_score'] > confidence_threshold]\n",
        "        \n",
        "        predicted_rois = []\n",
        "        for i, row in result.iterrows():\n",
        "            d = row['predict_rois']\n",
        "            d['class'] = row['predict_class']\n",
        "            predicted_rois.append(d)\n",
        "\n",
        "        return show_bounding_boxes(image_path, predicted_rois), result"
      ],
      "metadata": {
        "id": "7-V0cmdpq2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amls = []\n",
        "\n",
        "for d in datasets:\n",
        "    aml = AutoGluonObjectDetectionAML()\n",
        "    aml.set_dataset(d)\n",
        "    amls.append(aml)"
      ],
      "metadata": {
        "id": "736ydOQtssoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting\n",
        "\n",
        "!rm -r outputs\n",
        "\n",
        "for aml in amls:\n",
        "    aml.fit()"
      ],
      "metadata": {
        "id": "YlMFeAeOtD5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating\n",
        "\n",
        "for aml in amls:\n",
        "    aml.evaluate_test()"
      ],
      "metadata": {
        "id": "BmAu6wHmtiJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "img, result = amls[0].predict_on_random_sample(0.5)\n",
        "display(img)"
      ],
      "metadata": {
        "id": "YGWky_Z3v5dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, result = amls[1].predict_on_random_sample(0.5)\n",
        "display(img)"
      ],
      "metadata": {
        "id": "g7c2cx2uwRDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gb49PJXDftQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}