{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "xTSK0xXimOWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FP2c5a9Vu9c"
      },
      "source": [
        "# Install (restart runtime after this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrzD5Fx0qPas"
      },
      "outputs": [],
      "source": [
        "!pip install autokeras\n",
        "!pip install autogluon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "-ZhqOaWYUAbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "nr5EubYB6Y7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "zO9nhOInlHAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your kaggle API token into files panel, then run the cell\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "BerqRGjy6g-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r datasets\n",
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "6VWf4cFz7IyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download datasets from kaggle\n",
        "!kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection -p datasets --unzip\n",
        "!rm -r datasets/yes datasets/no\n",
        "!kaggle datasets download -d die9origephit/children-vs-adults-images -p datasets/child_vs_adult --unzip\n",
        "!kaggle datasets download -d dhruvildave/english-handwritten-characters-dataset -p datasets/english_handwritten_char --unzip\n",
        "# !kaggle datasets download -d hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images -p datasets --unzip\n",
        "# !mv datasets/Shoe\\ vs\\ Sandal\\ vs\\ Boot\\ Dataset datasets/show_sandal_boot\n",
        "# !kaggle datasets download -d muratkokludataset/rice-image-dataset -p datasets --unzip\n",
        "# !kaggle datasets download -d plameneduardo/sarscov2-ctscan-dataset -p datasets/covid_ct_scan --unzip"
      ],
      "metadata": {
        "id": "mXCYbLKt6sC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## index functions"
      ],
      "metadata": {
        "id": "_wCJOhWSlJ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS_ROOT = 'datasets'\n",
        "INDEX_DIR = '_index'\n",
        "CLASS = 'class'\n",
        "LABEL = 'label'\n",
        "PATH = 'image'"
      ],
      "metadata": {
        "id": "alB5QRXwXsGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image(file_name):\n",
        "    file_name = file_name.lower()\n",
        "    extensions = ['.png', '.jpg', '.jpeg']\n",
        "    for ex in extensions:\n",
        "        if file_name.endswith(ex):\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "Xe41xAwpUKNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_based_samples(dataset_dir, classes):\n",
        "    items = []\n",
        "    for c in classes:\n",
        "        c_dir = dataset_dir / c\n",
        "        for f in c_dir.glob('*'):\n",
        "            if not f.is_dir() and is_image(str(f)):\n",
        "                items.append({CLASS: c, PATH: str(f)})\n",
        "            if not f.is_dir() and not is_image(str(f)):\n",
        "                print(f)\n",
        "    return items"
      ],
      "metadata": {
        "id": "h6HGLBBhin4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def brain_tumor_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'brain_tumor_dataset'\n",
        "    classes = ['yes', 'no']\n",
        "    return get_class_based_samples(dataset_dir, classes)"
      ],
      "metadata": {
        "id": "DCUKDK4WTxpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def child_vs_adult_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/die9origephit/children-vs-adults-images\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'child_vs_adult'\n",
        "    classes = ['adults', 'children']\n",
        "    splits = ['train', 'test']\n",
        "    items = []\n",
        "    for sp in splits:\n",
        "        items += get_class_based_samples(dataset_dir / sp, classes)\n",
        "    return items"
      ],
      "metadata": {
        "id": "wkQB6Nhee4ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def english_handwritten_char_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/dhruvildave/english-handwritten-characters-dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'english_handwritten_char'\n",
        "    index_df = pd.read_csv(str(dataset_dir / 'english.csv'))\n",
        "    index_df = index_df.rename(columns={'image': PATH, 'label': CLASS})\n",
        "    index_df[PATH] = index_df[PATH].apply(lambda p: os.path.join(dataset_dir, p))\n",
        "    return index_df.to_dict('records')"
      ],
      "metadata": {
        "id": "Rp2WJCGJf47I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sandal_boot_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'show_sandal_boot'\n",
        "    classes = ['Boot', 'Sandal', 'Shoe']\n",
        "    return get_class_based_samples(dataset_dir, classes)"
      ],
      "metadata": {
        "id": "0WOFuEoKiQej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rice_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'Rice_Image_Dataset'\n",
        "    classes = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']\n",
        "    return get_class_based_samples(dataset_dir, classes)"
      ],
      "metadata": {
        "id": "sHc7vCzcjm5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def covid_ct_scan_samples():\n",
        "    \"\"\"\n",
        "    link: https://www.kaggle.com/datasets/plameneduardo/sarscov2-ctscan-dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = Path(DATASETS_ROOT) / 'covid_ct_scan'\n",
        "    classes = ['COVID', 'non-COVID']\n",
        "    return get_class_based_samples(dataset_dir, classes)"
      ],
      "metadata": {
        "id": "EJLLgUVokXvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset object"
      ],
      "metadata": {
        "id": "P5l0La43lT8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, name, samples_generator_function, split_test_ratio):\n",
        "        self.name = name\n",
        "\n",
        "        self.index_df = pd.DataFrame(samples_generator_function())\n",
        "        self.classes = self.index_df[CLASS].unique().tolist()\n",
        "        self.index_df[LABEL] = self.index_df[CLASS].apply(\n",
        "            lambda c: self.classes.index(c))\n",
        "\n",
        "        self.split_test_ratio = split_test_ratio\n",
        "        self.index_dir = str(Path(DATASETS_ROOT) / INDEX_DIR / name)\n",
        "    \n",
        "    def make_splits(self):\n",
        "        stratify = self.index_df[CLASS].values\n",
        "        train, test = train_test_split(\n",
        "            self.index_df, test_size=self.split_test_ratio,\n",
        "            shuffle=True, stratify=stratify)\n",
        "        os.makedirs(self.index_dir, exist_ok=True)\n",
        "        train.to_csv(os.path.join(self.index_dir, 'train.csv'), index=False)\n",
        "        test.to_csv(os.path.join(self.index_dir, 'test.csv'), index=False)\n",
        "\n",
        "    @property\n",
        "    def train_csv_path(self):\n",
        "        return os.path.join(self.index_dir, 'train.csv')\n",
        "\n",
        "    @property\n",
        "    def test_csv_path(self):\n",
        "        return os.path.join(self.index_dir, 'test.csv')\n",
        "\n",
        "    def train_test_index_df(self):\n",
        "        return (\n",
        "            pd.read_csv(self.train_csv_path), \n",
        "            pd.read_csv(self.test_csv_path)\n",
        "        )"
      ],
      "metadata": {
        "id": "YJ76384fdTtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "    Dataset('brain_tumor', brain_tumor_samples, 0.3),\n",
        "    Dataset('child_vs_adult', child_vs_adult_samples, 0.2),\n",
        "    # Dataset('english_hand_written_char', english_handwritten_char_samples, 0.15),\n",
        "    # Dataset('show_sandal_boot', show_sandal_boot_samples, 0.1),\n",
        "    # Dataset('rice', rice_samples, 0.1),\n",
        "    # Dataset('covid_ct_scan', covid_ct_scan_samples, 0.15),\n",
        "]"
      ],
      "metadata": {
        "id": "v7qmnuh3elJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in datasets:\n",
        "    d.make_splits()"
      ],
      "metadata": {
        "id": "hYkhThhNexXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement systems wrapper"
      ],
      "metadata": {
        "id": "B7doRu7cI876"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoMLSystem:\n",
        "    def set_dataset(self, dataset):\n",
        "        pass\n",
        "        \n",
        "    def fit(self, time_budget):\n",
        "        pass\n",
        "    \n",
        "    def predict_test(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "2_ihubqaJDwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Gluon"
      ],
      "metadata": {
        "id": "w3M5E2hisJw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImagePredictor, ImageDataset\n",
        "\n",
        "\n",
        "class AutoGluonImageClassiferAML(AutoMLSystem):\n",
        "    def set_dataset(self, dataset):\n",
        "        self.dataset = dataset        \n",
        "        self.train_dataset = ImageDataset(\n",
        "            dataset.train_csv_path, dataset.classes, image_column=PATH)\n",
        "        self.test_dataset = ImageDataset(\n",
        "            dataset.test_csv_path, dataset.classes, image_column=PATH)\n",
        "        \n",
        "    def fit(self, time_budget):\n",
        "        self.aml = ImagePredictor(\n",
        "            path=os.path.join('outputs', 'ag', str(time_budget), self.dataset.name))\n",
        "        # since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio\n",
        "        self.aml.fit(self.train_dataset, time_limit=time_budget,\n",
        "                     presets='medium_quality_faster_train')\n",
        "        print('fit_summary:', self.aml.fit_summary())\n",
        "    \n",
        "    def predict_test(self):\n",
        "        labels = self.test_dataset[LABEL]\n",
        "        return self.aml.predict(self.test_dataset), labels"
      ],
      "metadata": {
        "id": "EcDwLHkBsLpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets[0]\n",
        "agaml = AutoGluonImageClassiferAML()\n",
        "agaml.set_dataset(dataset)\n",
        "agaml.fit(60)\n",
        "p, l = agaml.predict_test()\n",
        "# p, l"
      ],
      "metadata": {
        "id": "9q1rWm9pLBx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agaml.aml.list_models()"
      ],
      "metadata": {
        "id": "LItVIiYz7650"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Keras"
      ],
      "metadata": {
        "id": "hslDxBq6sEj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r datasets/_tf"
      ],
      "metadata": {
        "id": "9lkh4VzCcEFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autokeras as ak\n",
        "import time\n",
        "\n",
        "\n",
        "class AutoKerasImageClassiferAML(AutoMLSystem):\n",
        "    def set_dataset(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "        def convert_to_tf_dataset_compatible_dir():\n",
        "            new_dataset_dir = Path(DATASETS_ROOT) / '_tf' / dataset.name\n",
        "            if new_dataset_dir.exists():\n",
        "                return new_dataset_dir / 'train', new_dataset_dir / 'test'\n",
        "\n",
        "            train_df, test_df = dataset.train_test_index_df()\n",
        "            for sp, df in zip(['train', 'test'], [train_df, test_df]):\n",
        "                sp_dir = new_dataset_dir / sp\n",
        "                for i, r in df.iterrows():\n",
        "                    class_dir = sp_dir / r[CLASS]\n",
        "                    class_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy(r[PATH], class_dir)\n",
        "            return new_dataset_dir / 'train', new_dataset_dir / 'test'\n",
        "\n",
        "        train_dir, test_dir = convert_to_tf_dataset_compatible_dir()\n",
        "        image_size = (256, 256)\n",
        "\n",
        "        self.train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            train_dir, class_names=dataset.classes,\n",
        "            batch_size=None, image_size=image_size)\n",
        "        self.test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            test_dir, class_names=dataset.classes,\n",
        "            batch_size=None, image_size=image_size, shuffle=False)\n",
        "        \n",
        "    def fit(self, time_budget):\n",
        "        start_time = time.time()        \n",
        "        elapsed_time = 0\n",
        "        \n",
        "        while elapsed_time < time_budget:\n",
        "            print(f'start new trial ...')\n",
        "            self.aml = ak.ImageClassifier(\n",
        "                project_name=self.dataset.name, max_trials=2,\n",
        "                directory=os.path.join('outputs', 'ak', str(time_budget)),\n",
        "                overwrite=False)\n",
        "            self.aml.fit(self.train_dataset, epochs=20, verbose=True)\n",
        "            # self.aml.fit(self.train_dataset, verbose=True)\n",
        "            elapsed_time = int(time.time() - start_time)\n",
        "            print('elapsed time:', elapsed_time)\n",
        "    \n",
        "    def predict_test(self):\n",
        "        labels = self.test_dataset.map(lambda x, y: y)\n",
        "        labels_list = [l.numpy() for l in labels]\n",
        "        return self.aml.predict(self.test_dataset).reshape(-1).astype(int), np.array(labels_list)"
      ],
      "metadata": {
        "id": "xlbQU7wLhL-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets[0]\n",
        "akaml = AutoKerasImageClassiferAML()\n",
        "akaml.set_dataset(dataset)\n",
        "akaml.fit(10)\n",
        "akaml.predict_test()"
      ],
      "metadata": {
        "id": "DVknFUT8iON1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking"
      ],
      "metadata": {
        "id": "gRdvdYbdozLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r outputs"
      ],
      "metadata": {
        "id": "oDmol3I9wj0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKtXYdrpVd9w"
      },
      "outputs": [],
      "source": [
        "systems_cls = [AutoGluonImageClassiferAML, AutoKerasImageClassiferAML]\n",
        "time_budgets = [5*60, 15*60]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_train_time_h = sum(time_budgets) * len(systems_cls) * len(datasets) / 3600\n",
        "print('Minimum required time (h):', min_train_time_h)"
      ],
      "metadata": {
        "id": "pfMmoW4CpAnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score\n",
        ")\n",
        "\n",
        "def calculate_metrics(predictions, labels):\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(labels, predictions)\n",
        "    metrics['f1_macro'] = f1_score(labels, predictions, average='macro')\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "_izs0wSgpC76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def run(system_cls, time_budget, dataset):\n",
        "    system_name = system_cls.__name__\n",
        "    result = {\n",
        "        'system': system_name,\n",
        "        'budget': time_budget,\n",
        "        'dataset': dataset.name,\n",
        "        'status': 'failed'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print('start loading system ...')\n",
        "        t = time.time()\n",
        "        aml = system_cls()\n",
        "        aml.set_dataset(dataset)\n",
        "        result['load_time'] = time.time() - t\n",
        "\n",
        "        print('start training ...')\n",
        "        t = time.time()\n",
        "        aml.fit(time_budget)\n",
        "        result['train_time'] = time.time() - t\n",
        "\n",
        "        print('start predicting ...')\n",
        "        t = time.time()\n",
        "        predictions, labels = aml.predict_test()\n",
        "        result['inference_time'] = time.time() - t\n",
        "        \n",
        "        print('caculating metrics ...')\n",
        "        metrics = calculate_metrics(predictions, labels)\n",
        "        for m in metrics:\n",
        "            result[f'metric_{m}'] = metrics[m]\n",
        "\n",
        "        result['status'] = 'success'\n",
        "        \n",
        "    except Exception as e:\n",
        "        print('EXCEPTION:', e)\n",
        "        result['exception'] = str(e)\n",
        "    \n",
        "    print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "7u0MaWd3vItf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "\n",
        "for b in time_budgets:\n",
        "    for dataset in datasets:\n",
        "        for sys in systems_cls:\n",
        "            r = run(sys, b, dataset)\n",
        "            all_results.append(r)\n",
        "            pd.DataFrame(all_results).to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "7m66JT8ZvQJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('results.csv') "
      ],
      "metadata": {
        "id": "Spk4F8kXvT1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result analysis"
      ],
      "metadata": {
        "id": "cSi_lbnwhkbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.plotting.backend = 'plotly'\n",
        "\n",
        "results_df = pd.read_csv('results.csv')\n",
        "results_df"
      ],
      "metadata": {
        "id": "7V3eoFuYv_HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [c for c in results_df.columns if c.startswith('metric')]\n",
        "\n",
        "for metric in metrics:\n",
        "    display(results_df.groupby('system')[metric].mean().T.plot(kind='bar', barmode='group', title=f'{metric}'))"
      ],
      "metadata": {
        "id": "lQn6dHFnhsbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.groupby(['dataset'])['metric_f1_macro'].std()"
      ],
      "metadata": {
        "id": "g4y-kS9hmnKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quality_metrics = ['metric_f1_macro']\n",
        "results_df['quality'] = results_df[quality_metrics].fillna(0).max(axis=1)\n",
        "results_df"
      ],
      "metadata": {
        "id": "6TQNQmiEr-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def covert_setting_index_to_col(series, main_col):\n",
        "    series_df = series.to_frame()\n",
        "    series_df.columns = ['unk']\n",
        "    series_df = series_df.reset_index()\n",
        "    series_df = pd.DataFrame(list(series_df.apply(\n",
        "        lambda r: {main_col: r[main_col], r['system']: r['unk']}, axis=1).values))\n",
        "    return series_df.groupby(main_col).max()"
      ],
      "metadata": {
        "id": "h_-lLIZnswbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = covert_setting_index_to_col(results_df.groupby(['system', 'budget'])['quality'].mean(), 'budget')\n",
        "d.T.plot(kind='bar', barmode='group')"
      ],
      "metadata": {
        "id": "8_PYDrfHsyql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdcoOd8Ms0dV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0FP2c5a9Vu9c",
        "zO9nhOInlHAF",
        "_wCJOhWSlJ7D",
        "B7doRu7cI876"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}